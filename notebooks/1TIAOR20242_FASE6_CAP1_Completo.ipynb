{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "43cc36c9",
      "metadata": {
        "id": "43cc36c9"
      },
      "source": [
        "# Detec√ß√£o de Ve√≠culos com YOLOv5 Adaptado (Treinamento e Avalia√ß√£o)  \n",
        "### Referente √† Entrega 1 do Projeto PBL - Fase 6"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dlZgVEjGUUJh",
      "metadata": {
        "id": "dlZgVEjGUUJh"
      },
      "source": [
        "## PBL Fase 6 ‚Äì Vis√£o Computacional com YOLOv5"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Q01fp5q0VfrQ",
      "metadata": {
        "id": "Q01fp5q0VfrQ"
      },
      "source": [
        "### Reconhecimento de Ve√≠culos: Carros e Motos\n",
        "\n",
        "Este notebook apresenta o desenvolvimento de um sistema de vis√£o computacional utilizando a arquitetura YOLOv5 para detectar e classificar dois tipos de ve√≠culos: carros e motos. A atividade faz parte da Fase 6 do Projeto Baseado em Problemas (PBL) da FIAP, no contexto da empresa fict√≠cia FarmTech Solutions, que est√° explorando o uso de intelig√™ncia artificial em aplica√ß√µes de seguran√ßa patrimonial e automa√ß√£o.\n",
        "\n",
        "O objetivo principal √© treinar um modelo YOLOv5 com base em imagens rotuladas manualmente e avaliar seu desempenho na detec√ß√£o dos objetos escolhidos. O projeto envolve tamb√©m a compara√ß√£o de resultados com diferentes quantidades de √©pocas de treinamento.\n",
        "\n",
        "### Objetivos do notebook\n",
        "\n",
        "- Organizar e utilizar um dataset rotulado manualmente.\n",
        "- Treinar um modelo YOLOv5 com 30 e 60 √©pocas.\n",
        "- Avaliar acur√°cia, perda e desempenho.\n",
        "- Realizar infer√™ncia em imagens de teste.\n",
        "- Apresentar conclus√µes com base nos resultados obtidos.\n",
        "\n",
        "### Aquisi√ß√£o e prepara√ß√£o do dataset\n",
        "\n",
        "Para este projeto, foram utilizadas imagens de dois objetos distintos: **carros** e **motos**. As imagens foram obtidas manualmente a partir de pesquisas com licenciamento livre (Creative Commons) e screenshots, priorizando variedade de √¢ngulos e contextos.\n",
        "\n",
        "Ap√≥s a coleta, foram selecionadas:\n",
        "\n",
        "- 40 imagens de carros\n",
        "- 40 imagens de motos\n",
        "\n",
        "As imagens foram rotuladas manualmente utilizando a plataforma [MakeSense.ai](https://www.makesense.ai/), onde foram desenhadas as bounding boxes para cada objeto identificado e atribu√≠dos os r√≥tulos correspondentes.\n",
        "\n",
        "A estrutura final do dataset foi organizada conforme o padr√£o exigido pelo YOLOv5, separando as imagens e seus respectivos r√≥tulos em tr√™s conjuntos:\n",
        "\n",
        "- **Treinamento**: 32 imagens de cada classe (total de 64)\n",
        "- **Valida√ß√£o**: 4 imagens de cada classe (total de 8)\n",
        "- **Teste**: 4 imagens de cada classe (total de 8)\n",
        "\n",
        "As pastas est√£o organizadas da seguinte forma:\n",
        "\n",
        "```\n",
        "üì¶ 1TIAOR20242_FASE6_CAP1\n",
        "‚îÇ‚îÄ‚îÄ üìÅ dataset_images               # Pasta principal contendo imagens e labels\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ üìÅ images                   # Subpasta com as imagens divididas em conjuntos\n",
        "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ train                # Imagens utilizadas no treinamento (64 imagens)\n",
        "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ val                  # Imagens utilizadas na valida√ß√£o (8 imagens)\n",
        "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ test                 # Imagens utilizadas para avalia√ß√£o final (8 imagens)\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ üìÅ labels                   # Subpasta com os arquivos de r√≥tulo no formato YOLO\n",
        "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ train                # Labels correspondentes √†s imagens de treinamento\n",
        "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ val                  # Labels correspondentes √†s imagens de valida√ß√£o\n",
        "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ test                 # Labels correspondentes √†s imagens de teste\n",
        "‚îÇ‚îÄ‚îÄ üìÑ veiculos.yaml                # Arquivo de configura√ß√£o que define caminhos, n√∫mero de classes e seus nomes\n",
        "```\n",
        "### Estrutura deste notebook\n",
        "\n",
        "1. Verifica√ß√£o da disponibilidade de GPU\n",
        "2. Conex√£o com o Google Drive e prepara√ß√£o do ambiente\n",
        "3. Clonagem do reposit√≥rio YOLOv5 e instala√ß√£o das depend√™ncias\n",
        "4. Treinamento do modelo com 30 √©pocas\n",
        "5. Treinamento do modelo com 60 √©pocas\n",
        "6. Compara√ß√£o dos resultados\n",
        "7. Infer√™ncia no conjunto de teste\n",
        "8. Conclus√µes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "p3PYwHLEGF8I",
      "metadata": {
        "id": "p3PYwHLEGF8I"
      },
      "source": [
        "### Verifica√ß√£o da disponibilidade de GPU\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IC3lJ-TAVY4o",
      "metadata": {
        "id": "IC3lJ-TAVY4o"
      },
      "source": [
        "Antes de prosseguir com a montagem do ambiente e instala√ß√£o das depend√™ncias, √© importante verificar se o ambiente Colab est√° configurado com GPU.\n",
        "\n",
        "O uso da GPU √© altamente recomendado para o treinamento de modelos de detec√ß√£o de objetos, pois reduz significativamente o tempo de execu√ß√£o. Caso a GPU n√£o esteja habilitada, ser√° necess√°rio alterar o tipo de ambiente e reiniciar o notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "h6Tvt95DGI_5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6Tvt95DGI_5",
        "outputId": "5d541cd6-7841-4c9b-e585-37754007260b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device_name = torch.cuda.get_device_name(0)\n",
        "    print(\"\\033[92m‚úÖ GPU dispon√≠vel:\\033[0m\", device_name)\n",
        "    print(\"O ambiente est√° configurado corretamente. Voc√™ pode prosseguir com as pr√≥ximas etapas.\")\n",
        "else:\n",
        "    print(\"\\033[91m‚ùå ATEN√á√ÉO: GPU n√£o dispon√≠vel. O notebook est√° executando em CPU.\\033[0m\\n\")\n",
        "    print(\"‚ùó O treinamento com YOLOv5 em CPU pode ser extremamente lento e est√° sujeito a interrup√ß√µes por tempo limite.\")\n",
        "    print(\"\\n\\033[1mRecomenda√ß√µes:\\033[0m\")\n",
        "    print(\"- V√° at√© o menu 'Ambiente de execu√ß√£o > Alterar tipo de ambiente de execu√ß√£o'\")\n",
        "    print(\"- Selecione 'GPU' como acelerador de hardware e clique em 'Salvar'\")\n",
        "    print(\"- O ambiente ser√° reiniciado. Ap√≥s a reinicializa√ß√£o, reexecute esta c√©lula antes de continuar.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12bec74d",
      "metadata": {
        "id": "12bec74d"
      },
      "source": [
        "### Conex√£o com o Google Drive\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_9ni0qhYVTn0",
      "metadata": {
        "id": "_9ni0qhYVTn0"
      },
      "source": [
        "Nesta etapa, vamos conectar o ambiente do Google Colab com a conta do Google Drive onde o dataset e os arquivos de configura√ß√£o est√£o armazenados.\n",
        "\n",
        "Essa conex√£o permite que o notebook tenha acesso direto aos arquivos necess√°rios para o treinamento, valida√ß√£o e testes do modelo, como imagens, r√≥tulos e o arquivo de configura√ß√£o `.yaml`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0efc998",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0efc998",
        "outputId": "01e53572-c0fd-4c92-faff-7a357b384c92"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Verifica se o ambiente √© Google Colab ou local\n",
        "try:\n",
        "    # Verifica se est√° no Google Colab\n",
        "    from google.colab import drive\n",
        "    print(\"‚úÖ Executando no Google Colab\")\n",
        "\n",
        "    # Verifica se o Google Drive j√° est√° montado\n",
        "    if not os.path.exists('/content/drive/MyDrive'):\n",
        "        print(\"üîÑ Google Drive n√£o est√° montado. Montando agora...\")\n",
        "        drive.mount('/content/drive')\n",
        "    else:\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "        print(\"‚úÖ Google Drive j√° est√° montado.\")\n",
        "\n",
        "    # Caminho no Google Drive\n",
        "    dataset_dir = \"/content/drive/MyDrive/1TIAOR20242_FASE6_CAP1\"\n",
        "    yaml_filename = \"veiculos.yaml\"\n",
        "\n",
        "    # Verifica se o arquivo veiculos.yaml existe\n",
        "    if not os.path.exists(f\"{dataset_dir}/veiculos.yaml\"):\n",
        "        raise FileNotFoundError(f\"Arquivo n√£o encontrado: {dataset_dir}/veiculos.yaml\")\n",
        "except ImportError:\n",
        "    # Caso n√£o esteja no Colab, executa localmente\n",
        "    print(\"‚úÖ Executando localmente\")\n",
        "\n",
        "    # Caminho local\n",
        "    dataset_dir = os.path.abspath(\"../\")\n",
        "    yaml_filename = \"veiculos_local.yaml\"\n",
        "\n",
        "    # Verifica se o arquivo veiculos.yaml existe\n",
        "    if not os.path.exists(f\"{dataset_dir}/configuration/{yaml_filename}\"):\n",
        "        # Ajusta o caminho descendo um n√≠vel\n",
        "        dataset_dir = os.path.abspath(\"../../\")\n",
        "        if not os.path.exists(f\"{dataset_dir}/configuration/{yaml_filename}\"):\n",
        "            raise FileNotFoundError(f\"Arquivo n√£o encontrado: {dataset_dir}/configuration/{yaml_filename}\")\n",
        "\n",
        "# Exemplo de uso do caminho\n",
        "print(f\"Caminho dos Datasets: {dataset_dir}\")\n",
        "print(f\"Nome do Arquivo para Yolo: {yaml_filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZX9x4sVDCyuB",
      "metadata": {
        "id": "ZX9x4sVDCyuB"
      },
      "source": [
        "### Clonagem do reposit√≥rio YOLOv5\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "521e9c4a",
      "metadata": {
        "id": "521e9c4a"
      },
      "source": [
        "Nesta etapa, clonamos o reposit√≥rio oficial do YOLOv5, mantido pela Ultralytics. Esse reposit√≥rio cont√©m os scripts de treinamento, valida√ß√£o e infer√™ncia, al√©m de arquivos de configura√ß√£o e exemplos pr√°ticos.\n",
        "\n",
        "A clonagem cria uma c√≥pia local do reposit√≥rio no ambiente de execu√ß√£o do Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1da43792",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1da43792",
        "outputId": "5e771606-dfe4-4bc8-82d6-721f8e0428bc"
      },
      "outputs": [],
      "source": [
        "# Clonar o reposit√≥rio oficial do YOLOv5\n",
        "!git clone https://github.com/ultralytics/yolov5"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NlgwgearDRHx",
      "metadata": {
        "id": "NlgwgearDRHx"
      },
      "source": [
        "### Instala√ß√£o das depend√™ncias\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pnMx6poMVCaI",
      "metadata": {
        "id": "pnMx6poMVCaI"
      },
      "source": [
        "Ap√≥s a clonagem do reposit√≥rio, √© necess√°rio instalar as depend√™ncias listadas no arquivo `requirements.txt`, presente no diret√≥rio do YOLOv5.\n",
        "\n",
        "Essas bibliotecas s√£o respons√°veis por garantir o funcionamento adequado dos scripts de treinamento e infer√™ncia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "168c8b56",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "168c8b56",
        "outputId": "0e46092c-600d-4b3c-a2e9-c2340be4c95b"
      },
      "outputs": [],
      "source": [
        "# !pip install -r requirements.txt\n",
        "import subprocess\n",
        "\n",
        "def install_dependencies():\n",
        "    try:\n",
        "        # Executa o comando de instala√ß√£o e suprime a sa√≠da\n",
        "        result = subprocess.run(\n",
        "            [\"pip\", \"install\", \"-r\", \"yolov5/requirements.txt\"],\n",
        "            stdout=subprocess.PIPE,  # Suprime a sa√≠da padr√£o\n",
        "            stderr=subprocess.PIPE,  # Suprime a sa√≠da de erro\n",
        "            text=True,\n",
        "            check=True  # Levanta uma exce√ß√£o se o comando falhar\n",
        "        )\n",
        "        print(\"‚úÖ Instala√ß√£o conclu√≠da com sucesso!\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(\"‚ùå Erro durante a instala√ß√£o.\")\n",
        "        print(f\"Detalhes do erro: {e.stderr}\")\n",
        "\n",
        "# Chama a fun√ß√£o para instalar as depend√™ncias\n",
        "install_dependencies()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "upOADmFjDhdd",
      "metadata": {
        "id": "upOADmFjDhdd"
      },
      "source": [
        "### Treinamento do modelo com 30 √©pocas (YOLOv5 adaptado)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "w340HulCU9Dx",
      "metadata": {
        "id": "w340HulCU9Dx"
      },
      "source": [
        "Nesta etapa ser√° realizado o primeiro treinamento do modelo YOLOv5 com 30 √©pocas, utilizando o dataset rotulado manualmente com imagens de carros e motos.\n",
        "\n",
        "Este modelo ser√° utilizado posteriormente na compara√ß√£o com um segundo treinamento, com 60 √©pocas, a fim de observar os efeitos do tempo de treinamento na precis√£o e no desempenho do modelo.\n",
        "\n",
        "As m√©tricas e o tempo de execu√ß√£o ser√£o armazenados para an√°lise comparativa entre os dois modelos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "194661c6",
      "metadata": {
        "id": "194661c6",
        "outputId": "1ca68e3e-2917-416d-dd68-90800a7993f8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "print(f'{dataset_dir}/{yaml_filename}')\n",
        "print(os.path.exists(f'{dataset_dir}/configuration/{yaml_filename}'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fc4b2d0",
      "metadata": {
        "id": "1fc4b2d0",
        "outputId": "f945fe7d-7281-4fe6-8044-c2e1ad70afd3"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Caminho do arquivo de log\n",
        "nome_treinamento = \"veiculos_yolo_30ep\"\n",
        "\n",
        "log_file_path = f'{nome_treinamento}.log'\n",
        "\n",
        "# Par√¢metros utilizados:\n",
        "# --img: define o tamanho das imagens de entrada (640x640 pixels)\n",
        "# --batch: n√∫mero de imagens processadas por vez (tamanho do lote)\n",
        "# --epochs: n√∫mero total de √©pocas de treinamento\n",
        "# --data: caminho do arquivo .yaml com o dataset e classes\n",
        "# --weights: modelo base pr√©-treinado (YOLOv5s)\n",
        "# --name: nome da pasta onde os resultados ser√£o salvos\n",
        "\n",
        "# Comando de treinamento\n",
        "command = [\n",
        "    \"python\", \"./yolov5/train.py\",\n",
        "    \"--img\", \"640\",\n",
        "    \"--batch\", \"16\",\n",
        "    \"--epochs\", \"30\",\n",
        "    \"--data\", f\"{dataset_dir}/configuration/{yaml_filename}\",\n",
        "    \"--weights\", \"yolov5s.pt\",\n",
        "    \"--name\", f\"{nome_treinamento}\"\n",
        "]\n",
        "\n",
        "# Fun√ß√£o para executar o comando e monitorar o processo\n",
        "def executar_treinamento(command, log_file_path):\n",
        "    with open(log_file_path, \"w\") as log_file:\n",
        "        try:\n",
        "            # Inicia o processo\n",
        "            process = subprocess.Popen(\n",
        "                command,\n",
        "                stdout=log_file,  # Redireciona stdout para o arquivo de log\n",
        "                stderr=log_file,  # Redireciona stderr para o arquivo de log\n",
        "                text=True\n",
        "            )\n",
        "\n",
        "            print(f\"Treinamento {nome_treinamento} iniciado. Acompanhe o progresso no arquivo de log.\")\n",
        "\n",
        "            # Verifica se o processo ainda est√° em execu√ß√£o\n",
        "            pontos = \"\"  # String para acumular os pontos\n",
        "            while process.poll() is None:\n",
        "                pontos += \".\"  # Adiciona um ponto a cada itera√ß√£o\n",
        "                print(f\"Treinamento em andamento{pontos}\", end=\"\\r\")  # Atualiza na mesma linha\n",
        "                time.sleep(10)  # Aguarda 10 segundos antes de verificar novamente\n",
        "\n",
        "            # Verifica o c√≥digo de sa√≠da\n",
        "            if process.returncode == 0:\n",
        "                print(f\"Treinamento {nome_treinamento} conclu√≠do com sucesso!\")\n",
        "            else:\n",
        "                print(f\"Erro durante o treinamento {nome_treinamento}. Verifique o log: {log_file_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao executar o comando: {e}\")\n",
        "\n",
        "\n",
        "# Marca o tempo de in√≠cio do treinamento\n",
        "inicio = time.time()\n",
        "\n",
        "# Executa o treinamento\n",
        "executar_treinamento(command, f'./logs/{log_file_path}')\n",
        "\n",
        "# Marca o tempo de t√©rmino e calcula a dura√ß√£o\n",
        "fim = time.time()\n",
        "duracao = fim - inicio\n",
        "print(f\"Tempo de treinamento (30 √©pocas): {duracao:.2f} segundos\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pBRAUifnED0F",
      "metadata": {
        "id": "pBRAUifnED0F"
      },
      "source": [
        "#### An√°lise dos resultados do treinamento (30 √©pocas)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Uj6OS5m2U5mw",
      "metadata": {
        "id": "Uj6OS5m2U5mw"
      },
      "source": [
        "A imagem gerada pelo YOLOv5 cont√©m as curvas de perda (loss), precis√£o, recall, mAP e outras m√©tricas relevantes coletadas durante o processo de treinamento.\n",
        "\n",
        "Essa visualiza√ß√£o ajuda a identificar se o modelo est√° convergindo corretamente e a verificar sinais de overfitting ou subajuste."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6d10b1a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6d10b1a",
        "outputId": "dd0b94a9-5dd9-4aa1-e45a-a7881368eded"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "# Exibe o gr√°fico de m√©tricas do treinamento\n",
        "display(Image(filename=f'./yolov5/runs/train/{nome_treinamento}/results.png', width=1200))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DtWp75P-L-6l",
      "metadata": {
        "id": "DtWp75P-L-6l"
      },
      "source": [
        "#### Infer√™ncia no conjunto de teste (modelo com 30 √©pocas)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SEe55-uNU18Q",
      "metadata": {
        "id": "SEe55-uNU18Q"
      },
      "source": [
        "Nesta etapa, utilizamos o modelo YOLOv5 treinado com 30 √©pocas para realizar infer√™ncia nas imagens do conjunto de teste. O objetivo √© observar o desempenho do modelo em imagens n√£o vistas anteriormente.\n",
        "\n",
        "O tempo total de execu√ß√£o ser√° registrado para fins comparativos com outras vers√µes e abordagens, conforme exigido na Entrega 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29ace33a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29ace33a",
        "outputId": "fea668a9-4b8c-44e0-8e51-97ce2c51112d"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Par√¢metros utilizados:\n",
        "# --weights: caminho para o modelo treinado (melhor vers√£o salva durante o treino de 30 √©pocas)\n",
        "# --img: tamanho das imagens usadas na infer√™ncia (em pixels)\n",
        "# --conf: n√≠vel m√≠nimo de confian√ßa para exibir uma detec√ß√£o\n",
        "# --source: diret√≥rio com as imagens de teste\n",
        "# --name: nome da pasta de sa√≠da (dentro de runs/detect)\n",
        "\n",
        "# Marca o in√≠cio da infer√™ncia\n",
        "inicio = time.time()\n",
        "\n",
        "# Executa a infer√™ncia com o modelo treinado\n",
        "!python ./yolov5/detect.py \\\n",
        "  --weights ./yolov5/runs/train/{nome_treinamento}/weights/best.pt \\\n",
        "  --img 640 \\\n",
        "  --conf 0.25 \\\n",
        "  --source {dataset_dir}/dataset_images/images/test \\\n",
        "  --name {nome_treinamento}_test\n",
        "\n",
        "# Marca o fim e calcula a dura√ß√£o\n",
        "fim = time.time()\n",
        "duracao = fim - inicio\n",
        "\n",
        "# Exibe o tempo total de infer√™ncia\n",
        "print(f\"Tempo de infer√™ncia no conjunto de teste: {duracao:.2f} segundos\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IQ8Q9JayO5E1",
      "metadata": {
        "id": "IQ8Q9JayO5E1"
      },
      "source": [
        "#### Visualiza√ß√£o dos resultados da infer√™ncia\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nf7Gjp8yUuvw",
      "metadata": {
        "id": "nf7Gjp8yUuvw"
      },
      "source": [
        "A seguir, s√£o exibidas algumas das imagens do conjunto de teste processadas pelo modelo treinado com 30 √©pocas. Os resultados mostram as bounding boxes geradas e as classes detectadas com suas respectivas confian√ßas.\n",
        "\n",
        "Essas evid√™ncias visuais s√£o importantes para demonstrar o funcionamento real do modelo e ser√£o utilizadas na an√°lise comparativa com o modelo treinado com 60 √©pocas e as outras abordagens previstas na Entrega 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "X_MITFyCO61t",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_MITFyCO61t",
        "outputId": "256ff064-80db-4cfa-c513-090b609a736d"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "from IPython.display import Image, display\n",
        "\n",
        "# Lista os arquivos resultantes da infer√™ncia\n",
        "resultado_imgs = glob.glob(f'./yolov5/runs/detect/{nome_treinamento}_test/*.jpg')\n",
        "\n",
        "# Exibe at√© 4 imagens para visualiza√ß√£o\n",
        "for img_path in resultado_imgs[:4]:\n",
        "    display(Image(filename=img_path, width=500))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dgoQVI2ePLbV",
      "metadata": {
        "id": "dgoQVI2ePLbV"
      },
      "source": [
        "#### Conclus√£o preliminar do modelo treinado com 30 √©pocas\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZtCM9GRBUlqs",
      "metadata": {
        "id": "ZtCM9GRBUlqs"
      },
      "source": [
        "Com base na execu√ß√£o realizada, o modelo YOLOv5 treinado com 30 √©pocas apresentou os seguintes resultados:\n",
        "\n",
        "- O tempo total de treinamento foi de aproximadamente **192 segundos**, utilizando a GPU do Colab.\n",
        "- O tempo de infer√™ncia no conjunto de teste foi de **13,35 segundos**, com m√©dia de aproximadamente **26,9 ms por imagem** para infer√™ncia e **21,6 ms** para NMS.\n",
        "- A an√°lise do gr√°fico `results.png` mostra uma boa converg√™ncia dos erros (loss), com redu√ß√£o progressiva e sem sinais de overfitting vis√≠veis. Os valores de precis√£o e recall tamb√©m aumentaram de forma consistente.\n",
        "- A m√©trica **mAP@0.5** se aproximou de **0.9**, o que indica excelente capacidade de detec√ß√£o em um cen√°rio com duas classes apenas.\n",
        "- O modelo identificou corretamente **6 de 8 imagens** no conjunto de teste, mantendo uma taxa pr√°tica de acerto de **75%**.\n",
        "- As duas imagens que n√£o apresentaram detec√ß√µes possivelmente envolvem limita√ß√µes naturais do modelo ou caracter√≠sticas espec√≠ficas das imagens, como √¢ngulos desfavor√°veis ou baixa resolu√ß√£o.\n",
        "\n",
        "Estes resultados servir√£o como base de compara√ß√£o para o pr√≥ximo experimento, com 60 √©pocas, permitindo avaliar se o aumento no n√∫mero de ciclos de treinamento traz ganhos relevantes em desempenho."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aT5dkH86QP7V",
      "metadata": {
        "id": "aT5dkH86QP7V"
      },
      "source": [
        "### Treinamento do modelo com 60 √©pocas (YOLOv5 adaptado)\n",
        "\n",
        "Nesta etapa, ser√° realizado um segundo treinamento do modelo YOLOv5 utilizando o mesmo dataset e os mesmos par√¢metros anteriores, exceto pelo n√∫mero de √©pocas, que foi ampliado para 60.\n",
        "\n",
        "O objetivo √© observar se o aumento no tempo de treinamento resulta em melhorias nas m√©tricas de desempenho e na capacidade de generaliza√ß√£o do modelo. O tempo de execu√ß√£o ser√° registrado para fins comparativos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cfee827",
      "metadata": {
        "id": "7cfee827",
        "outputId": "bef9f04b-c109-4bd4-fd87-79b299567a29"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Caminho do arquivo de log\n",
        "nome_treinamento = \"veiculos_yolo_60ep\"\n",
        "\n",
        "log_file_path = f'{nome_treinamento}.log'\n",
        "\n",
        "# Par√¢metros utilizados:\n",
        "# --img: define o tamanho das imagens de entrada (640x640 pixels)\n",
        "# --batch: n√∫mero de imagens processadas por vez (tamanho do lote)\n",
        "# --epochs: n√∫mero total de √©pocas de treinamento\n",
        "# --data: caminho do arquivo .yaml com o dataset e classes\n",
        "# --weights: modelo base pr√©-treinado (YOLOv5s)\n",
        "# --name: nome da pasta onde os resultados ser√£o salvos\n",
        "\n",
        "# Comando de treinamento\n",
        "command = [\n",
        "    \"python\", \"./yolov5/train.py\",\n",
        "    \"--img\", \"640\",\n",
        "    \"--batch\", \"16\",\n",
        "    \"--epochs\", \"60\",\n",
        "    \"--data\", f\"{dataset_dir}/configuration/{yaml_filename}\",\n",
        "    \"--weights\", \"yolov5s.pt\",\n",
        "    \"--name\", f\"{nome_treinamento}\"\n",
        "]\n",
        "\n",
        "# Fun√ß√£o para executar o comando e monitorar o processo\n",
        "def executar_treinamento(command, log_file_path):\n",
        "    with open(log_file_path, \"w\") as log_file:\n",
        "        try:\n",
        "            # Inicia o processo\n",
        "            process = subprocess.Popen(\n",
        "                command,\n",
        "                stdout=log_file,  # Redireciona stdout para o arquivo de log\n",
        "                stderr=log_file,  # Redireciona stderr para o arquivo de log\n",
        "                text=True\n",
        "            )\n",
        "\n",
        "            print(f\"Treinamento {nome_treinamento} iniciado. Acompanhe o progresso no arquivo de log.\")\n",
        "\n",
        "            # Verifica se o processo ainda est√° em execu√ß√£o\n",
        "            pontos = \"\"  # String para acumular os pontos\n",
        "            while process.poll() is None:\n",
        "                pontos += \".\"  # Adiciona um ponto a cada itera√ß√£o\n",
        "                print(f\"Treinamento em andamento{pontos}\", end=\"\\r\")  # Atualiza na mesma linha\n",
        "                time.sleep(10)  # Aguarda 10 segundos antes de verificar novamente\n",
        "\n",
        "            # Verifica o c√≥digo de sa√≠da\n",
        "            if process.returncode == 0:\n",
        "                print(f\"Treinamento {nome_treinamento} conclu√≠do com sucesso!\")\n",
        "            else:\n",
        "                print(f\"Erro durante o treinamento {nome_treinamento}. Verifique o log: {log_file_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao executar o comando: {e}\")\n",
        "\n",
        "\n",
        "# Marca o tempo de in√≠cio do treinamento\n",
        "inicio = time.time()\n",
        "\n",
        "# Executa o treinamento\n",
        "executar_treinamento(command, f'./logs/{log_file_path}')\n",
        "\n",
        "# Marca o tempo de t√©rmino e calcula a dura√ß√£o\n",
        "fim = time.time()\n",
        "duracao = fim - inicio\n",
        "print(f\"Tempo de treinamento (60 √©pocas): {duracao:.2f} segundos\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "V1UQAoL4Rp61",
      "metadata": {
        "id": "V1UQAoL4Rp61"
      },
      "source": [
        "#### An√°lise dos resultados do treinamento (60 √©pocas)\n",
        "\n",
        "A imagem gerada pelo YOLOv5 a partir do treinamento com 60 √©pocas mostra a evolu√ß√£o das m√©tricas de perda, precis√£o, recall e mAP. Essa visualiza√ß√£o √© importante para comparar a estabilidade, a converg√™ncia e o poss√≠vel ganho de desempenho em rela√ß√£o ao modelo treinado com 30 √©pocas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Zduj4DHpRst9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zduj4DHpRst9",
        "outputId": "bb71bac6-c4bd-42a0-8373-e996ebbca059"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "# Exibe o gr√°fico de m√©tricas do treinamento com 60 √©pocas\n",
        "display(Image(filename=f'./yolov5/runs/train/{nome_treinamento}/results.png', width=1200))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "o1NDgHdrR5wl",
      "metadata": {
        "id": "o1NDgHdrR5wl"
      },
      "source": [
        "#### Infer√™ncia no conjunto de teste (modelo com 60 √©pocas)\n",
        "\n",
        "Utilizando agora o modelo treinado com 60 √©pocas, faremos a infer√™ncia nas mesmas imagens de teste, para fins de compara√ß√£o direta com os resultados obtidos anteriormente com 30 √©pocas. O tempo de execu√ß√£o ser√° registrado, e os resultados visuais ser√£o armazenados para an√°lise.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RZmuR5fOR8FA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZmuR5fOR8FA",
        "outputId": "cdac8616-4316-48d3-dd76-21ddb28360c2"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Par√¢metros utilizados:\n",
        "# --weights: modelo treinado com 60 √©pocas\n",
        "# --img: tamanho das imagens de entrada\n",
        "# --conf: confian√ßa m√≠nima para exibir uma detec√ß√£o\n",
        "# --source: diret√≥rio com imagens de teste\n",
        "# --name: nome da pasta de sa√≠da dos resultados\n",
        "\n",
        "# Marca o in√≠cio da infer√™ncia\n",
        "inicio = time.time()\n",
        "\n",
        "# Executa a infer√™ncia\n",
        "!python ./yolov5/detect.py \\\n",
        "  --weights ./yolov5/runs/train/{nome_treinamento}/weights/best.pt \\\n",
        "  --img 640 \\\n",
        "  --conf 0.25 \\\n",
        "  --source {dataset_dir}/dataset_images/images/test \\\n",
        "  --name {nome_treinamento}_test\n",
        "\n",
        "# Marca o fim e calcula a dura√ß√£o\n",
        "fim = time.time()\n",
        "duracao = fim - inicio\n",
        "\n",
        "# Exibe o tempo total de infer√™ncia\n",
        "print(f\"Tempo de infer√™ncia no conjunto de teste: {duracao:.2f} segundos\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2mEZnWPxSI90",
      "metadata": {
        "id": "2mEZnWPxSI90"
      },
      "source": [
        "#### Visualiza√ß√£o das detec√ß√µes (modelo com 60 √©pocas)\n",
        "\n",
        "A seguir, s√£o exibidas algumas das imagens do conjunto de teste processadas pelo modelo treinado com 60 √©pocas. A ideia √© comparar visualmente a qualidade das detec√ß√µes em rela√ß√£o ao modelo anterior e observar se houve ganho percept√≠vel em precis√£o ou consist√™ncia.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sUG1NEBESKpo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUG1NEBESKpo",
        "outputId": "f6fbcbf0-8384-4fc6-c9cc-35e253dea9da"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "from IPython.display import Image, display\n",
        "\n",
        "# Lista os arquivos resultantes da infer√™ncia\n",
        "resultado_imgs = glob.glob(f'./yolov5/runs/detect/{nome_treinamento}_test/*.jpg')\n",
        "\n",
        "# Exibe at√© 4 imagens\n",
        "for img_path in resultado_imgs[:4]:\n",
        "    display(Image(filename=img_path, width=500))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YFdDH97kS-CM",
      "metadata": {
        "id": "YFdDH97kS-CM"
      },
      "source": [
        "#### Conclus√£o preliminar do modelo treinado com 60 √©pocas\n",
        "\n",
        "Com base na execu√ß√£o realizada, o modelo YOLOv5 treinado com 60 √©pocas apresentou os seguintes resultados:\n",
        "\n",
        "- O tempo total de treinamento foi de aproximadamente **268 segundos**, tamb√©m utilizando a GPU do Colab.\n",
        "- O tempo de infer√™ncia no conjunto de teste foi de **7,72 segundos**, com m√©dia de aproximadamente **24,4 ms por imagem** para infer√™ncia e **18,5 ms** para NMS.\n",
        "- A an√°lise do gr√°fico `results.png` mostra uma converg√™ncia ainda mais est√°vel das perdas (loss), com quedas suaves e cont√≠nuas, e sem sinais de sobreajuste (overfitting).\n",
        "- As m√©tricas **precision** e **recall** alcan√ßaram valores pr√≥ximos de 1.0, com boa estabilidade ao longo das √©pocas.\n",
        "- A m√©trica **mAP@0.5** se manteve pr√≥xima de 1.0 nas √∫ltimas √©pocas, enquanto a **mAP@0.5:0.95** apresentou evolu√ß√£o e menor oscila√ß√£o em rela√ß√£o ao modelo anterior.\n",
        "- O modelo identificou corretamente **7 de 8 imagens** no conjunto de teste, incluindo um caso com **dupla detec√ß√£o correta (Carro e Moto)**, resultando em uma taxa pr√°tica de acerto de **87,5%**.\n",
        "- Apenas uma imagem n√£o apresentou detec√ß√£o, mantendo a consist√™ncia geral e sugerindo boa capacidade de generaliza√ß√£o.\n",
        "\n",
        "Esses resultados indicam um desempenho superior ao modelo de 30 √©pocas, tanto em termos quantitativos quanto qualitativos, e posicionam o modelo de 60 √©pocas como refer√™ncia para compara√ß√£o com as demais abordagens previstas na Entrega 2.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_J2ld48BTmkV",
      "metadata": {
        "id": "_J2ld48BTmkV"
      },
      "source": [
        "### Compara√ß√£o entre modelos: 30 vs 60 √©pocas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bc3a542",
      "metadata": {
        "id": "4bc3a542"
      },
      "source": [
        "Ap√≥s o treinamento e a avalia√ß√£o dos dois modelos YOLOv5 adaptados, √© poss√≠vel estabelecer uma compara√ß√£o baseada em m√©tricas quantitativas e qualitativas.\n",
        "\n",
        "### Tempo de treinamento\n",
        "\n",
        "- **30 √©pocas**: aproximadamente 192 segundos\n",
        "- **60 √©pocas**: aproximadamente 268 segundos  \n",
        "‚Üí A duplica√ß√£o das √©pocas resultou em um tempo 39% maior, o que √© esperado e aceit√°vel no contexto.\n",
        "\n",
        "### Tempo de infer√™ncia\n",
        "\n",
        "- **30 √©pocas**: 13,35 segundos (‚âà26,9 ms/infer√™ncia + 21,6 ms/NMS)\n",
        "- **60 √©pocas**: 7,72 segundos (‚âà24,4 ms/infer√™ncia + 18,5 ms/NMS)  \n",
        "‚Üí O modelo de 60 √©pocas foi ligeiramente mais r√°pido, indicando otimiza√ß√£o na arquitetura gerada.\n",
        "\n",
        "### Detec√ß√£o nas imagens de teste\n",
        "\n",
        "- **30 √©pocas**:\n",
        "  - 6 de 8 imagens corretamente detectadas\n",
        "  - 2 imagens sem detec√ß√£o\n",
        "- **60 √©pocas**:\n",
        "  - 7 de 8 imagens corretamente detectadas\n",
        "  - Uma das imagens apresentou detec√ß√£o de **duas classes corretamente (Carro e Moto)**  \n",
        "‚Üí Houve melhora clara na cobertura e sensibilidade do modelo.\n",
        "\n",
        "### M√©tricas gr√°ficas (resultados.png)\n",
        "\n",
        "- **30 √©pocas**:\n",
        "  - mAP@0.5: pr√≥ximo de 0.9\n",
        "  - mAP@0.5:0.95: ~0.6\n",
        "- **60 √©pocas**:\n",
        "  - mAP@0.5: estabilizado em 1.0\n",
        "  - mAP@0.5:0.95: pr√≥xima de 0.7 com menos flutua√ß√µes  \n",
        "‚Üí Indica melhor generaliza√ß√£o e menor varia√ß√£o nas m√©tricas.\n",
        "\n",
        "### Conclus√£o\n",
        "\n",
        "O modelo treinado com **60 √©pocas apresentou resultados superiores** em praticamente todos os aspectos: detec√ß√£o pr√°tica, estabilidade gr√°fica e precis√£o nas m√©tricas. Apesar do tempo de treinamento maior, os ganhos justificam seu uso como vers√£o final do modelo para a pr√≥xima fase do projeto.\n",
        "\n",
        "Essas observa√ß√µes ser√£o fundamentais na Entrega 2, onde este modelo ser√° comparado com outras abordagens, como a YOLO tradicional e uma CNN treinada do zero.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qOnmi3udTsmh",
      "metadata": {
        "id": "qOnmi3udTsmh"
      },
      "source": [
        "### Conclus√£o da Entrega 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ma8ceESPT6fk",
      "metadata": {
        "id": "ma8ceESPT6fk"
      },
      "source": [
        "Nesta primeira etapa do projeto, foi implementado um sistema de detec√ß√£o de objetos utilizando o modelo YOLOv5 adaptado, treinado com um dataset customizado de imagens de **carros** e **motos** rotulado manualmente.\n",
        "\n",
        "Dois experimentos foram realizados: um com **30 √©pocas** e outro com **60 √©pocas**. Ambos os modelos foram treinados e avaliados com base em m√©tricas quantitativas, desempenho pr√°tico na infer√™ncia e an√°lise visual dos resultados.\n",
        "\n",
        "Os principais achados foram:\n",
        "\n",
        "- O modelo com **60 √©pocas apresentou desempenho superior**, com maior taxa de acerto e m√©tricas mais est√°veis, mesmo com um tempo de treinamento apenas moderadamente maior.\n",
        "- A **diferen√ßa nas curvas de mAP e perda** entre os dois modelos indica um ganho relevante em generaliza√ß√£o e refinamento da detec√ß√£o.\n",
        "- As imagens de teste revelaram **melhor cobertura e confian√ßa** nas detec√ß√µes do modelo com mais √©pocas, inclusive detectando m√∫ltiplas classes em uma √∫nica imagem.\n",
        "\n",
        "A estrutura do notebook, os tempos de execu√ß√£o, as sa√≠das visuais e os resultados num√©ricos obtidos nesta etapa ser√£o utilizados como base de compara√ß√£o na pr√≥xima fase do projeto."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "S_flNrlrUJ9k",
      "metadata": {
        "id": "S_flNrlrUJ9k"
      },
      "source": [
        "## Transi√ß√£o para a Entrega 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "W4hNCvyzT9vA",
      "metadata": {
        "id": "W4hNCvyzT9vA"
      },
      "source": [
        "A partir da pr√≥xima etapa, ser√£o avaliadas outras abordagens de vis√£o computacional aplicadas ao mesmo problema, com o objetivo de comparar diferentes t√©cnicas de detec√ß√£o e classifica√ß√£o de imagens.\n",
        "\n",
        "As abordagens previstas s√£o:\n",
        "\n",
        "1. **YOLO tradicional (pr√©-treinado)** ‚Äî aplicado sem ajuste no dataset customizado;\n",
        "2. **CNN treinada do zero** ‚Äî com foco na tarefa de classifica√ß√£o entre carro e moto.\n",
        "\n",
        "Essas abordagens ser√£o analisadas com os mesmos crit√©rios definidos na Entrega 1:\n",
        "\n",
        "- Precis√£o e desempenho do modelo\n",
        "- Tempo de treinamento e infer√™ncia\n",
        "- Facilidade de uso e integra√ß√£o\n",
        "- Qualidade dos resultados visuais\n",
        "\n",
        "A seguir, ser√° iniciada a prepara√ß√£o para a implementa√ß√£o dessas duas novas abordagens.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c72f9130",
      "metadata": {
        "id": "c72f9130"
      },
      "source": [
        "# Compara√ß√£o entre Abordagens de Vis√£o Computacional (YOLOv5, YOLO Tradicional e CNN)  \n",
        "### Referente √† Entrega 2 do Projeto PBL - Fase 6\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vU4PY23Gb19F",
      "metadata": {
        "id": "vU4PY23Gb19F"
      },
      "source": [
        "Nesta segunda etapa, o objetivo √© comparar diferentes abordagens de vis√£o computacional aplicadas ao mesmo conjunto de dados utilizado na Entrega 1. A proposta √© identificar vantagens e limita√ß√µes entre t√©cnicas de detec√ß√£o e classifica√ß√£o baseadas em redes neurais.\n",
        "\n",
        "As abordagens implementadas s√£o:\n",
        "\n",
        "1. YOLOv5 Adaptado (j√° treinado na Entrega 1)\n",
        "2. YOLO Tradicional (modelo pr√©-treinado, sem ajustes)\n",
        "3. CNN desenvolvida do zero (classifica√ß√£o entre duas classes)\n",
        "\n",
        "A an√°lise comparativa ser√° baseada em crit√©rios como precis√£o, tempo de infer√™ncia, facilidade de implementa√ß√£o e aplicabilidade pr√°tica.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bd16326",
      "metadata": {
        "id": "6bd16326"
      },
      "source": [
        "### Refer√™ncia: Resultado da Entrega 1 (YOLOv5 Adaptado - 60 √©pocas)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sotR9rNFdhXI",
      "metadata": {
        "id": "sotR9rNFdhXI"
      },
      "source": [
        "Nesta etapa, utilizaremos como refer√™ncia comparativa o modelo YOLOv5 treinado com 60 √©pocas, desenvolvido na Entrega 1.\n",
        "\n",
        "Esse modelo obteve os seguintes resultados:\n",
        "\n",
        "- Tempo de treinamento: **268 segundos**\n",
        "- Tempo de infer√™ncia: **7,72 segundos**\n",
        "- Detec√ß√£o correta em **7 de 8 imagens** do conjunto de teste\n",
        "- **mAP@0.5**: ~1.0\n",
        "- **mAP@0.5:0.95**: ~0.7\n",
        "\n",
        "Abaixo est√° o gr√°fico de m√©tricas gerado durante o treinamento deste modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acba5d0b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acba5d0b",
        "outputId": "bbad335e-65d7-4928-8a35-c51285bd9464"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "# Exibir gr√°fico de m√©tricas\n",
        "print(\"Gr√°fico de desempenho do modelo YOLOv5 (60 √©pocas):\")\n",
        "display(Image(filename=f'./yolov5/runs/train/{nome_treinamento}/results.png', width=1200))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54cc2964",
      "metadata": {
        "id": "54cc2964"
      },
      "source": [
        "## 1. Aplica√ß√£o da YOLOv5 Tradicional (Pr√©-treinada)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Gg367cMvqSNg",
      "metadata": {
        "id": "Gg367cMvqSNg"
      },
      "source": [
        "Nesta abordagem, utilizamos o modelo `yolov5s.pt` fornecido pela Ultralytics, sem ajustes no dataset customizado.\n",
        "\n",
        "Este modelo j√° foi treinado em um grande conjunto de dados p√∫blicos (COCO dataset), o que lhe permite detectar diversas classes gen√©ricas. O objetivo aqui √© avaliar se ele √© capaz de identificar **carros** e **motos** em nosso conjunto de teste, mesmo sem ter sido adaptado para essas imagens espec√≠ficas.\n",
        "\n",
        "Esta etapa tamb√©m permite comparar como um modelo gen√©rico se comporta frente a um modelo customizado, como o desenvolvido na Entrega 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80246785",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80246785",
        "outputId": "d3be2011-3c21-4ad2-e4f8-e78c2c40055e"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "nome_treinamento = \"yolo_tradicional\"\n",
        "\n",
        "# Par√¢metros utilizados:\n",
        "# --weights: define o modelo YOLOv5 a ser utilizado (pr√©-treinado 'yolov5s.pt')\n",
        "# --img: tamanho da imagem de entrada (640x640 pixels)\n",
        "# --conf: n√≠vel m√≠nimo de confian√ßa para considerar uma detec√ß√£o\n",
        "# --source: diret√≥rio com as imagens do conjunto de teste\n",
        "# --name: nome da pasta onde os resultados da infer√™ncia ser√£o salvos (dentro de runs/detect/)\n",
        "\n",
        "# Marca o in√≠cio da execu√ß√£o\n",
        "inicio = time.time()\n",
        "\n",
        "# Executa a infer√™ncia com o modelo pr√©-treinado\n",
        "!python ./yolov5/detect.py \\\n",
        "  --weights yolov5s.pt \\\n",
        "  --img 640 \\\n",
        "  --conf 0.25 \\\n",
        "  --source {dataset_dir}/dataset_images/images/test \\\n",
        "  --name {nome_treinamento}_test\n",
        "\n",
        "# Marca o fim da execu√ß√£o e calcula a dura√ß√£o\n",
        "fim = time.time()\n",
        "print(f\"Tempo de infer√™ncia com YOLO pr√©-treinado: {fim - inicio:.2f} segundos\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Wv1w3f1Ae_Q8",
      "metadata": {
        "id": "Wv1w3f1Ae_Q8"
      },
      "source": [
        "### Visualiza√ß√£o dos resultados da YOLOv5 Tradicional\n",
        "\n",
        "Abaixo est√£o algumas imagens do conjunto de teste processadas pelo modelo pr√©-treinado YOLOv5s. O objetivo √© observar quais objetos foram detectados, como foram classificados e se h√° consist√™ncia nas detec√ß√µes para as classes de interesse (carro e moto).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UCb2-u_afByw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UCb2-u_afByw",
        "outputId": "c0f7ea68-21a5-41bf-e2eb-0ab8e40fe093"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "import glob\n",
        "\n",
        "# Local dos resultados gerados pelo YOLO pr√©-treinado\n",
        "imagens_yolo_tradicional = glob.glob(f'./yolov5/runs/detect/{nome_treinamento}_test/*.jpg')\n",
        "\n",
        "# Exibir at√© 4 imagens\n",
        "for img_path in imagens_yolo_tradicional[:4]:\n",
        "    display(Image(filename=img_path, width=500))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uwK8elyNfJuU",
      "metadata": {
        "id": "uwK8elyNfJuU"
      },
      "source": [
        "### An√°lise de desempenho do modelo YOLOv5 Tradicional (Pr√©-treinado)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "I3mH-FZ0fMu0",
      "metadata": {
        "id": "I3mH-FZ0fMu0"
      },
      "source": [
        "\n",
        "Ao aplicar o modelo YOLOv5s pr√©-treinado (sem re-treinamento) sobre as imagens do conjunto de teste, foram observadas as seguintes caracter√≠sticas e comportamentos:\n",
        "\n",
        "#### üîπ Detec√ß√µes realizadas\n",
        "- O modelo foi capaz de detectar **carros** e **motos**, que s√£o classes presentes no dataset COCO (base de treinamento do YOLOv5 pr√©-treinado).\n",
        "- Em algumas imagens, foi capaz de detectar **mais de um objeto da mesma classe**, como m√∫ltiplos carros ou motos, demonstrando uma boa capacidade de identifica√ß√£o em ambientes com m√∫ltiplas ocorr√™ncias.\n",
        "- Algumas imagens apresentaram **detec√ß√µes incorretas ou irrelevantes**, como a classifica√ß√£o de \"train\" ou \"truck\", indicando limita√ß√µes no foco da aplica√ß√£o.\n",
        "\n",
        "#### üîπ Acertos e limita√ß√µes por imagem\n",
        "- **Imagem 073**: Detec√ß√£o de 3 carros e 1 moto, indicando boa sensibilidade.\n",
        "- **Imagem 074**: Detec√ß√£o incorreta como \"train\", devido √† vista traseira incomum do fusca.\n",
        "- **Imagem 075**: Detec√ß√£o correta de um carro.\n",
        "- **Imagem 076**: Detec√ß√£o como \"truck\" (possivelmente confundido com o design do Tesla Cybertruck).\n",
        "- **Imagem 077**: Detec√ß√£o correta de uma moto.\n",
        "- **Imagem 078**: Detec√ß√£o de 4 motos, indicando alta sensibilidade, mas poss√≠vel excesso.\n",
        "- **Imagem 079**: Detec√ß√£o correta de uma moto.\n",
        "- **Imagem 080**: Nenhuma detec√ß√£o (moto tipo Vespa, possivelmente fora do padr√£o aprendido).\n",
        "\n",
        "#### üîπ Tempo de infer√™ncia\n",
        "- O tempo total de infer√™ncia foi de aproximadamente **7,84 segundos** para processar as 8 imagens do conjunto de teste.\n",
        "- A m√©dia de tempo por imagem foi de aproximadamente **23,8 ms de infer√™ncia** e **18,7 ms de NMS**, o que representa uma performance adequada para aplica√ß√µes em tempo real ou embarcadas.\n",
        "\n",
        "#### üîπ Robustez e generaliza√ß√£o\n",
        "- O modelo demonstrou **capacidade de generalizar** bem para contextos diversos, sendo capaz de detectar objetos mesmo em imagens n√£o padronizadas.\n",
        "- A presen√ßa de falsos positivos e detec√ß√µes de classes fora do escopo do projeto refor√ßa o car√°ter **gen√©rico** do modelo.\n",
        "\n",
        "Esses resultados demonstram que o modelo YOLOv5 pr√©-treinado, mesmo sem ajustes, consegue fornecer **resultados relevantes** em ambientes variados, desde que as classes estejam contempladas em seu conjunto de treinamento original.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lG2-GtmHqoHI",
      "metadata": {
        "id": "lG2-GtmHqoHI"
      },
      "source": [
        "### Conclus√£o parcial ‚Äì YOLOv5 Tradicional (Pr√©-treinado)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1eutZwrqm4B",
      "metadata": {
        "id": "c1eutZwrqm4B"
      },
      "source": [
        "O modelo YOLOv5s pr√©-treinado, utilizado sem re-treinamento, apresentou um desempenho satisfat√≥rio ao ser aplicado diretamente sobre as imagens do conjunto de teste do projeto.\n",
        "\n",
        "Mesmo sem ter sido treinado com o dataset espec√≠fico, o modelo foi capaz de detectar corretamente diversas ocorr√™ncias das classes \"car\" e \"motorcycle\", mostrando uma boa generaliza√ß√£o para objetos presentes em seu dataset original (COCO).\n",
        "\n",
        "Al√©m disso, o modelo demonstrou:\n",
        "- **Capacidade de identificar m√∫ltiplos objetos por imagem**, o que √© relevante em ambientes complexos.\n",
        "- **Tempo de infer√™ncia adequado** (7,84 segundos para 8 imagens), tornando-o vi√°vel para aplica√ß√µes pr√°ticas.\n",
        "- **Alguns falsos positivos ou classifica√ß√µes irrelevantes**, como \"train\" e \"truck\", esperados em um modelo gen√©rico.\n",
        "\n",
        "Por outro lado, a detec√ß√£o incorreta de objetos incomuns (como o Tesla Cybertruck ou a traseira aberta de um fusca) e a aus√™ncia de detec√ß√£o em casos menos padronizados (como a moto Vespa) indicam **limita√ß√µes relacionadas √† representatividade do dataset de origem** e √† falta de especializa√ß√£o no dom√≠nio do projeto.\n",
        "\n",
        "Em resumo, o YOLO pr√©-treinado √© uma ferramenta pr√°tica e eficaz para uso geral, oferecendo bons resultados iniciais mesmo sem personaliza√ß√£o. No entanto, **pode n√£o atender com a precis√£o exigida em contextos espec√≠ficos**, como os esperados por um cliente com necessidades claramente definidas.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10cfe198",
      "metadata": {
        "id": "10cfe198"
      },
      "source": [
        "## 2. Classifica√ß√£o com CNN do Zero (Carro vs Moto)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tb4w_meNrEo8",
      "metadata": {
        "id": "tb4w_meNrEo8"
      },
      "source": [
        "Nesta etapa, ser√° implementada uma rede neural convolucional simples, treinada do zero para classificar imagens entre duas categorias: \"carro\" e \"moto\".\n",
        "\n",
        "Diferente do modelo YOLO, que realiza detec√ß√£o de objetos, a CNN ser√° respons√°vel apenas pela **classifica√ß√£o da imagem como um todo**, com base em caracter√≠sticas visuais.\n",
        "\n",
        "Para isso, utilizaremos as mesmas imagens da Entrega 1, reestruturando o dataset para seguir o padr√£o esperado por bibliotecas como `ImageDataGenerator` (Keras) ou `ImageFolder` (PyTorch).\n",
        "\n",
        "A estrutura esperada do diret√≥rio de dados √© a seguinte:\n",
        "\n",
        "```\n",
        "üì¶ 1TIAOR20242_FASE6_CAP1\n",
        "‚îÇ‚îÄ‚îÄ üìÅ dataset_cnn           # Pasta principal para classifica√ß√£o com CNN\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ üìÅ train             # Imagens utilizadas para o treinamento (64 imagens por classe)\n",
        "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ carro         # Imagens da classe \"carro\"\n",
        "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ moto          # Imagens da classe \"moto\"\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ üìÅ val               # Imagens utilizadas para valida√ß√£o (8 imagens por classe)\n",
        "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ carro         # Imagens da classe \"carro\"\n",
        "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ moto          # Imagens da classe \"moto\"\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ üìÅ test              Imagens utilizadas para avalia√ß√£o final (8 imagens por classe)\n",
        "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ carro         # Imagens da classe \"carro\"\n",
        "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ moto          # Imagens da classe \"moto\"\n",
        "```\n",
        "\n",
        "Com essa organiza√ß√£o, √© poss√≠vel carregar as imagens automaticamente com os respectivos r√≥tulos, simplificando o processo de prepara√ß√£o do dataset para a CNN."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9YdW4IIwv5-d",
      "metadata": {
        "id": "9YdW4IIwv5-d"
      },
      "source": [
        "### Verifica√ß√£o e montagem do Google Drive\n",
        "\n",
        "Antes de carregar o dataset da CNN, verificamos se o Google Drive est√° montado no ambiente do Google Colab.\n",
        "\n",
        "Essa etapa √© essencial, pois o ambiente pode ter sido reiniciado e a conex√£o com o Drive perdida desde a execu√ß√£o da Entrega 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hACep1xAv8ol",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hACep1xAv8ol",
        "outputId": "550d81b8-cf35-4b96-b82c-da2071a7d364"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Verifica se o ambiente √© Google Colab ou local\n",
        "try:\n",
        "    # Verifica se est√° no Google Colab\n",
        "    from google.colab import drive\n",
        "    print(\"‚úÖ Executando no Google Colab\")\n",
        "\n",
        "    # Verifica se o Google Drive j√° est√° montado\n",
        "    if not os.path.exists('/content/drive/MyDrive'):\n",
        "        print(\"üîÑ Google Drive n√£o est√° montado. Montando agora...\")\n",
        "        drive.mount('/content/drive')\n",
        "    else:\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "        print(\"‚úÖ Google Drive j√° est√° montado.\")\n",
        "\n",
        "    # Caminho no Google Drive\n",
        "    dataset_dir = \"/content/drive/MyDrive/1TIAOR20242_FASE6_CAP1/dataset_cnn\"\n",
        "\n",
        "except ImportError:\n",
        "    # Caso n√£o esteja no Colab, executa localmente\n",
        "    print(\"‚úÖ Executando localmente\")\n",
        "\n",
        "    # Caminho local\n",
        "    dataset_dir = os.path.abspath(\"../dataset_cnn\")\n",
        "\n",
        "# Exemplo de uso do caminho\n",
        "print(f\"Caminho dos Datasets: {dataset_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FTuuaRw7vDRB",
      "metadata": {
        "id": "FTuuaRw7vDRB"
      },
      "source": [
        "### Carregamento do dataset a partir do Google Drive\n",
        "\n",
        "Com o dataset j√° estruturado na pasta `dataset_cnn` dentro do Google Drive, vamos utilizar a biblioteca `ImageDataGenerator` da Keras para carregar e preparar os dados para treinamento, valida√ß√£o e teste.\n",
        "\n",
        "O carregamento ser√° feito com base na estrutura de subpastas (`carro/` e `moto/`) j√° organizadas dentro dos diret√≥rios `train`, `val` e `test`.\n",
        "\n",
        "O caminho completo do dataset no Google Drive deve ser:\n",
        "```\n",
        "/content/drive/MyDrive/1TIAOR20242_FASE6_CAP1/dataset_cnn/\n",
        "```\n",
        "\n",
        "Cada subpasta dentro de `train/`, `val/` e `test/` representa o r√≥tulo da classe correspondente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1Zeo0YSMwwcM",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Zeo0YSMwwcM",
        "outputId": "64a2365b-c269-4ce7-95d0-632387e40a07"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Fun√ß√£o para validar as subpastas\n",
        "def validar_subpastas(dataset_dir):\n",
        "    subpastas_necessarias = ['test', 'val', 'train']\n",
        "    subpastas_existentes = os.listdir(dataset_dir) if os.path.exists(dataset_dir) else []\n",
        "\n",
        "    # Verifica se todas as subpastas necess√°rias est√£o presentes\n",
        "    subpastas_faltando = [pasta for pasta in subpastas_necessarias if pasta not in subpastas_existentes]\n",
        "\n",
        "    if subpastas_faltando:\n",
        "        print(\"\\033[91m‚ùå ERRO: As seguintes subpastas est√£o faltando no diret√≥rio:\\033[0m\")\n",
        "        for pasta in subpastas_faltando:\n",
        "            print(f\"  - {pasta}\")\n",
        "        print(\"\\033[93m‚ö†Ô∏è Verifique o caminho do dataset e a estrutura esperada.\\033[0m\")\n",
        "    else:\n",
        "        print(\"\\033[92m‚úÖ Todas as subpastas necess√°rias est√£o presentes:\\033[0m\")\n",
        "        for pasta in subpastas_necessarias:\n",
        "            print(f\"  - {pasta}\")\n",
        "\n",
        "# Valida as subpastas\n",
        "validar_subpastas(dataset_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a510f6d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a510f6d",
        "outputId": "f48af55e-c039-4b38-a282-394d10efcb6b"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Tamanhos e par√¢metros\n",
        "image_size = (224, 224)\n",
        "batch_size = 16\n",
        "\n",
        "# Pr√©-processamento b√°sico: reescala os pixels para o intervalo [0, 1]\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Carregamento dos dados\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    directory=f\"{dataset_dir}/train\",\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary'  # classifica√ß√£o bin√°ria\n",
        ")\n",
        "\n",
        "val_generator = datagen.flow_from_directory(\n",
        "    directory=f\"{dataset_dir}/val\",\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "test_generator = datagen.flow_from_directory(\n",
        "    directory=f\"{dataset_dir}/test\",\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    shuffle=False  # importante para avalia√ß√£o\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3xQ5-qt71FDc",
      "metadata": {
        "id": "3xQ5-qt71FDc"
      },
      "source": [
        "## Defini√ß√£o da CNN (Rede Neural Convolucional)\n",
        "\n",
        "A rede definida a seguir tem como objetivo classificar imagens entre duas categorias: \"carro\" e \"moto\".\n",
        "\n",
        "Trata-se de uma arquitetura simples, ideal para bases de dados menores como a utilizada neste projeto. A rede √© composta por:\n",
        "\n",
        "- Tr√™s blocos de camadas convolucionais + max pooling;\n",
        "- Camada de flatten (achatar os filtros);\n",
        "- Duas camadas densas (fully connected), sendo a √∫ltima com ativa√ß√£o sigmoide para classifica√ß√£o bin√°ria.\n",
        "\n",
        "A perda ser√° calculada com `binary_crossentropy`, e a m√©trica principal ser√° a **acur√°cia**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GP0k60W01Ghs",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "id": "GP0k60W01Ghs",
        "outputId": "b4ac56f8-e088-49a2-a528-5a8de5132b2f"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Arquitetura da CNN\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    Flatten(),\n",
        "\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Dense(1, activation='sigmoid')  # Sa√≠da bin√°ria: 0 ou 1\n",
        "])\n",
        "\n",
        "# Compila√ß√£o do modelo\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.0001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Resumo do modelo\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VU_ChSkq031A",
      "metadata": {
        "id": "VU_ChSkq031A"
      },
      "source": [
        "### Treinamento do modelo CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ymqRNe-S02fw",
      "metadata": {
        "id": "ymqRNe-S02fw"
      },
      "source": [
        "\n",
        "\n",
        "Com os dados devidamente organizados e carregados, o modelo convolucional ser√° treinado por um n√∫mero definido de √©pocas.\n",
        "\n",
        "Durante o treinamento, ser√£o observadas as m√©tricas de acur√°cia e perda (**loss**) tanto para o conjunto de **treinamento** quanto para o de **valida√ß√£o**, com o objetivo de identificar a evolu√ß√£o do aprendizado da rede e eventuais ind√≠cios de overfitting.\n",
        "\n",
        "Como o conjunto de dados √© relativamente pequeno, utilizaremos um n√∫mero moderado de √©pocas, evitando o risco de sobreajuste. Os resultados ser√£o registrados para posterior an√°lise comparativa com as abordagens baseadas em YOLO.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IqG3Owz53Amd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqG3Owz53Amd",
        "outputId": "635b3380-454e-4900-dc0e-a04696af535a"
      },
      "outputs": [],
      "source": [
        "print(\"üîç Dimens√£o do batch de treino:\", train_generator.image_shape)\n",
        "print(\"üî¢ Classes detectadas:\", train_generator.class_indices)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TJX9P8_X071s",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJX9P8_X071s",
        "outputId": "cb4a84e2-e9e2-449e-f505-93d18b69f2d9"
      },
      "outputs": [],
      "source": [
        "# Par√¢metros\n",
        "epochs = 30\n",
        "\n",
        "# Treinamento do modelo\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=epochs,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iF_EO9cB21Ro",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "iF_EO9cB21Ro",
        "outputId": "bd4a82da-6932-48d4-b642-b382f1ab62a3"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_training_history(history):\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    epochs_range = range(len(acc))\n",
        "\n",
        "    plt.figure(figsize=(14, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs_range, acc, label='Acur√°cia Treino')\n",
        "    plt.plot(epochs_range, val_acc, label='Acur√°cia Valida√ß√£o')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.title('Acur√°cia durante o treinamento')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs_range, loss, label='Perda Treino')\n",
        "    plt.plot(epochs_range, val_loss, label='Perda Valida√ß√£o')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.title('Perda durante o treinamento')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "plot_training_history(history)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "837ac955",
      "metadata": {
        "id": "837ac955"
      },
      "source": [
        "## COMPARATIVO DOS MODELOS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f880be2",
      "metadata": {
        "id": "1f880be2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97f9e297",
      "metadata": {
        "id": "97f9e297"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f5bf461",
      "metadata": {
        "id": "9f5bf461"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
